{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9237cf",
   "metadata": {},
   "source": [
    "# 第7章 AdaBoost元算法\n",
    "\n",
    "元算法（meta-algorithm）是对其他算法进行组合的一种方式，AdaBoost分类器是目前最流行的元算法。\n",
    "\n",
    "再接下来，我们就会建立一个单层决策树（decision stump）分类器。实际上，它是一个单节点的决策树。AdaBoost算法将应用在上述单层决策树分类器之上。我们将在一个难数据集上应用AdaBoost分类器，以了解该算法是如何迅速超越其他分类器的。\n",
    "\n",
    "> 优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。\n",
    ">\n",
    "> 缺点：对离群点敏感。\n",
    ">\n",
    "> 适用数据类型：数值型和标称型数据。\n",
    "\n",
    "## 7.1 基于数据集多重抽样的分类器\n",
    "\n",
    "我们可以将之前章节的不同的分类器组合起来，而这种组合结果则被称为集成方法（ensemble method）或者元算法（meta-algorithm）。使用集成方法时会有多种形式：可以是不同算法的集成，也可以是同一算法在不同设置(参数)下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。\n",
    "\n",
    "### 7.1.1 bagging\n",
    "\n",
    "自举汇聚法（bootstrap aggregating），也称为bagging方法，即：每次有放回的随机选取样本，组成新数据集，且新数据集与原数据集大小相等。\n",
    "\n",
    "这种自举允许重复数据，而某些数据在新数据中甚至没有出现。\n",
    "\n",
    "每个数据集选取N个样本，抽取S个数据集，然后每个训练集训练一个模型。\n",
    "\n",
    "将上步得到的S个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。\n",
    "\n",
    "### 7.1.2 boosting\n",
    "\n",
    "boosting在样本上与原始样本没区别，每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整。可将弱学习器提升为强学习器的算法。\n",
    "\n",
    "### 7.1.3 对比\n",
    "\n",
    "**样本选择上：**\n",
    "\n",
    "- Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。\n",
    "- Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。\n",
    "\n",
    "**样例权重：**\n",
    "\n",
    "- Bagging：使用均匀取样，每个样例的权重相等。\n",
    "- Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。\n",
    "\n",
    "**预测函数：**\n",
    "\n",
    "- Bagging：所有预测函数的权重相等。\n",
    "- Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。\n",
    "\n",
    "**并行计算：**\n",
    "\n",
    "- Bagging：各个预测函数可以并行生成。\n",
    "- Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。\n",
    "\n",
    "---\n",
    "\n",
    "这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。\n",
    "\n",
    "下面是将决策树与这些算法框架进行结合所得到的新的算法：\n",
    "\n",
    "Bagging + 决策树 = 随机森林\n",
    "\n",
    "AdaBoost + 决策树 = 提升树\n",
    "\n",
    "Gradient Boosting + 决策树 = GBDT\n",
    "\n",
    "集成方法众多，本文主要关注Boosting方法中的一种最流行的版本，即AdaBoost。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 7.2 训练算法\n",
    "\n",
    "AdaBoost是adaptive boosting（自适应boosting）的缩写，其运行过程如下：训练数据中的每个样本，并赋予其一个权重，这些权重构成了向量*D*。一开始，这些权重都初始化成相等值。\n",
    "\n",
    "首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。\n",
    "\n",
    "为了从所有弱分类器中得到最终的分类结果，AdaBoost为每个分类器都分配了一个权重值 $\\alpha$，这些$\\alpha$值是基于每个弱分类器的错误率进行计算的。其中，错误率*ε*的定义为： \n",
    "$$\n",
    "ε=\\frac{未正确分类的样本数目}{所有样本数目}\n",
    "$$\n",
    " $\\alpha$的计算公式如下：\n",
    "$$\n",
    "\\alpha=\\frac{1}{2}ln(\\frac{1-ε}{ε})\n",
    "$$\n",
    "\n",
    "> $\\frac{1-ε}{ε}=\\frac{正确分类数}{错误分类数}$，所以加上$ln$函数后，让正确占比大的数>0，错误占比大的分类器<0\n",
    "\n",
    "计算出alpha值之后，可以对权重向量*D*进行更新，以使得那些正确分类的样本的权重降低而错分样本的权重升高。*D*的计算方法如下:\n",
    "\n",
    "正确分类的样本权重更改为：\n",
    "$$\n",
    "D_i^{(t+1)}=\\frac{D_i^{(t)}e^{-\\alpha}}{Sum(D)}\n",
    "$$\n",
    " 错误样本的权重更改为：\n",
    "$$\n",
    "D_i^{(t+1)}=\\frac{D_i^{(t)}e^{\\alpha}}{Sum(D)}\n",
    "$$\n",
    "在计算出*D*之后，AdaBoost又开始进入下一轮迭代。AdaBoost算法会不断地重复训练和调整权重的过程，直到训练错误率为0或者弱分类器的数目达到用户的指定值为止。 \n",
    "\n",
    "## 7.3 单层决策树构建的弱分类器\n",
    "\n",
    "单层决策树（decision stump，也称决策树桩）是一种简单的决策树。接下来将构建一个单层决策树，而它仅基于单个特征来做决策。由于这棵树只有一次分裂过程，因此它实际上就是一个树桩。\n",
    "\n",
    "为了训练单层决策树，我们需要创建一个训练集，编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb9b125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def loadSimpData():\n",
    "    \"\"\"\n",
    "    创建单层决策树的数据集\n",
    "    Parameters:\n",
    "        无\n",
    "    Returns:\n",
    "        dataMat - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "    \"\"\"\n",
    "    datMat = np.matrix([[ 1. ,  2.1],\n",
    "        [ 1.5,  1.6],\n",
    "        [ 1.3,  1. ],\n",
    "        [ 1. ,  1. ],\n",
    "        [ 2. ,  1. ]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat,classLabels\n",
    "def showDataSet(dataMat, labelMat):\n",
    "    \"\"\"\n",
    "    数据可视化\n",
    "    Parameters:\n",
    "        dataMat - 数据矩阵\n",
    "        labelMat - 数据标签\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    data_plus = []                                  #正样本\n",
    "    data_minus = []                                 #负样本\n",
    "    for i in range(len(dataMat)):\n",
    "        if labelMat[i] > 0:\n",
    "            data_plus.append(dataMat[i])\n",
    "        else:\n",
    "            data_minus.append(dataMat[i])\n",
    "    data_plus_np = np.array(data_plus)                                             #转换为numpy矩阵\n",
    "    data_minus_np = np.array(data_minus)                                         #转换为numpy矩阵\n",
    "    plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1])        #正样本散点图\n",
    "    plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1])     #负样本散点图\n",
    "    plt.show()\n",
    "    \n",
    "dataArr,classLabels = loadSimpData()\n",
    "showDataSet(dataArr,classLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a89f6f",
   "metadata": {},
   "source": [
    "如果想要试着从某个坐标轴上选择一个值（即选择一条与坐标轴平行的直线$y=kx$或者$x=ky$）来将所有的蓝色圆点和橘色圆点分开，这显然是不可能的。\n",
    "\n",
    "这就是单层决策树难以处理的一个著名问题。通过使用多颗单层决策树，我们可以构建出一个能够对该数据集完全正确分类的分类器。\n",
    "\n",
    "下述代码的逻辑大致是这样的：\n",
    "\n",
    "1. 首先按数据集的特征进行遍历，给定一个阈值，计算当前特征下的数据点被阈值分类的准确情况\n",
    "2. 遍历所有特征和阈值，选出，这个非常弱的分类器最好的情况\n",
    "3. 注意⚠️，这里最好的情况，是指对某一类的区分就够了，而不是整体。比如说我对样本是1预测准确率很高，但是-1很低，没关系，我们就只关注1预测准的地方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b83514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: dim 0, thresh 0.90, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 0.90, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.00, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.00, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.10, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.10, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.20, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.20, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.30, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.30, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.40, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.40, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.50, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.50, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.60, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.60, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.70, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.70, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.80, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.80, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.90, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.90, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 2.00, thresh ineqal: lt, the weighted error is 0.600\n",
      "split: dim 0, thresh 2.00, thresh ineqal: gt, the weighted error is 0.400\n",
      "split: dim 1, thresh 0.89, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 0.89, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.00, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.00, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.11, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.11, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.22, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.22, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.33, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.33, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.44, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.44, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.55, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.55, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.66, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.66, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.77, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.77, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.88, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.88, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.99, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.99, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 2.10, thresh ineqal: lt, the weighted error is 0.600\n",
      "split: dim 1, thresh 2.10, thresh ineqal: gt, the weighted error is 0.400\n",
      "bestStump:\n",
      " {'dim': 0, 'thresh': 1.3, 'ineq': 'lt'}\n",
      "minError:\n",
      " [[0.2]]\n",
      "bestClasEst:\n",
      " [[-1.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "def loadSimpData():\n",
    "    \"\"\"\n",
    "    创建单层决策树的数据集\n",
    "    Parameters:\n",
    "        无\n",
    "    Returns:\n",
    "        dataMat - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "    \"\"\"\n",
    "    datMat = np.matrix([[ 1. ,  2.1],\n",
    "        [ 1.5,  1.6],\n",
    "        [ 1.3,  1. ],\n",
    "        [ 1. ,  1. ],\n",
    "        [ 2. ,  1. ]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat,classLabels\n",
    "\n",
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "    \"\"\"\n",
    "    单层决策树分类函数:依据阈值，非常弱智的判断矩阵中的值是否在阈值区间内；\n",
    "    Parameters:\n",
    "        dataMatrix - 数据矩阵\n",
    "        dimen - 第dimen列，也就是第几个特征\n",
    "        threshVal - 阈值\n",
    "        threshIneq - 标志\n",
    "    Returns:\n",
    "        retArray - 分类结果\n",
    "    \"\"\"\n",
    "    retArray = np.ones((np.shape(dataMatrix)[0],1))                #初始化retArray为1\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0         #如果小于阈值,则赋值为-1\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.0         #如果大于阈值,则赋值为-1\n",
    "    return retArray\n",
    "\n",
    "def buildStump(dataArr,classLabels,D):\n",
    "    \"\"\"\n",
    "    找到数据集上最佳的单层决策树\n",
    "    Parameters:\n",
    "        dataArr - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        D - 样本权重\n",
    "    Returns:\n",
    "        bestStump - 最佳单层决策树信息\n",
    "        minError - 最小误差\n",
    "        bestClasEst - 最佳的分类结果\n",
    "    \"\"\"\n",
    "    dataMatrix = np.mat(dataArr); labelMat = np.mat(classLabels).T\n",
    "    m,n = np.shape(dataMatrix)\n",
    "    numSteps = 10.0; bestStump = {}; bestClasEst = np.mat(np.zeros((m,1)))\n",
    "    # 最小误差初始\n",
    "    minError = float('inf')  \n",
    "    # 遍历所有特征\n",
    "    for i in range(n):                                                            \n",
    "        # 获取当前特征中最小的值和最大值\n",
    "        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max()        \n",
    "        # 步长\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps                                \n",
    "        for j in range(-1, int(numSteps) + 1):                                     \n",
    "            for inequal in ['lt', 'gt']:\n",
    "                threshVal = (rangeMin + float(j) * stepSize)                     #计算阈值\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)#计算分类结果\n",
    "                errArr = np.mat(np.ones((m,1)))                                 #初始化误差矩阵\n",
    "                errArr[predictedVals == labelMat] = 0                             #分类正确的,赋值为0\n",
    "                weightedError = D.T * errArr                                      #计算误差\n",
    "                print(\"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError))\n",
    "                if weightedError < minError:                                     #找到误差最小的分类方式\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump,minError,bestClasEst\n",
    "\n",
    "dataArr,classLabels = loadSimpData()\n",
    "D = np.mat(np.ones((5, 1)) / 5)\n",
    "bestStump,minError,bestClasEst = buildStump(dataArr,classLabels,D)\n",
    "print('bestStump:\\n', bestStump)\n",
    "print('minError:\\n', minError)\n",
    "print('bestClasEst:\\n', bestClasEst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f5687",
   "metadata": {},
   "source": [
    "从结果可以看出，预测最好的地方是：对于第一个特征，取阈值是1.3，判断都$\\le$1.3 是很准的,准确率达80%。\n",
    "\n",
    "接下来，使用AdaBoost算法提升分类器性能，将分类误差缩短到0，看下AdaBoost算法是如何实现的。\n",
    "\n",
    "## 7.4 完整AdaBoost算法\n",
    "\n",
    "下述代码的实现思路：\n",
    "1. 首先，初始化一个均等的和为1的数据集权重矩阵D；\n",
    "2. 依据D带入上节中的“弱学习器”中，返回当前矩阵D下最小错误率$ε$对应的的最优弱学习器参数；\n",
    "3. 依据错误率，带入公式求得当前弱学习器的权重$\\alpha$\n",
    "4. 依据弱学习器的学习结果与真实结果之间的是否相等，计算下一轮“弱学习器”用到的权重矩阵D：计算错误的样本权重更大，正确的更小；\n",
    "5. 不停迭代，直至错误率为0或者达到要求的迭代次数。\n",
    "\n",
    "> `alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))`,其中1e-16 指0.0000000000000001，避免此次计算达到完全正确，导致除0溢出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e5d5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: dim 0, thresh 0.90, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 0.90, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.00, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.00, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.10, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.10, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.20, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.20, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.30, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.30, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.40, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.40, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.50, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.50, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.60, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.60, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.70, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.70, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.80, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.80, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.90, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.90, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 2.00, thresh ineqal: lt, the weighted error is 0.600\n",
      "split: dim 0, thresh 2.00, thresh ineqal: gt, the weighted error is 0.400\n",
      "split: dim 1, thresh 0.89, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 0.89, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.00, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.00, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.11, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.11, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.22, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.22, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.33, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.33, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.44, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.44, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.55, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.55, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.66, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.66, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.77, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.77, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.88, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.88, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.99, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.99, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 2.10, thresh ineqal: lt, the weighted error is 0.600\n",
      "split: dim 1, thresh 2.10, thresh ineqal: gt, the weighted error is 0.400\n",
      "D: [[0.2 0.2 0.2 0.2 0.2]]\n",
      "classEst:  [[-1.  1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "total error:  0.2\n",
      "split: dim 0, thresh 0.90, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 0, thresh 0.90, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 0, thresh 1.00, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.00, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.10, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.10, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.20, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.20, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.30, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.30, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.40, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.40, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.50, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.50, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.60, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.60, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.70, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.70, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.80, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.80, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.90, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.90, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 2.00, thresh ineqal: lt, the weighted error is 0.750\n",
      "split: dim 0, thresh 2.00, thresh ineqal: gt, the weighted error is 0.250\n",
      "split: dim 1, thresh 0.89, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 0.89, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.00, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.00, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.11, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.11, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.22, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.22, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.33, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.33, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.44, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.44, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.55, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.55, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.66, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.66, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.77, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.77, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.88, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.88, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.99, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.99, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 2.10, thresh ineqal: lt, the weighted error is 0.750\n",
      "split: dim 1, thresh 2.10, thresh ineqal: gt, the weighted error is 0.250\n",
      "D: [[0.5   0.125 0.125 0.125 0.125]]\n",
      "classEst:  [[ 1.  1. -1. -1. -1.]]\n",
      "aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "total error:  0.2\n",
      "split: dim 0, thresh 0.90, thresh ineqal: lt, the weighted error is 0.143\n",
      "split: dim 0, thresh 0.90, thresh ineqal: gt, the weighted error is 0.857\n",
      "split: dim 0, thresh 1.00, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.00, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.10, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.10, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.20, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.20, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.30, thresh ineqal: lt, the weighted error is 0.286\n",
      "split: dim 0, thresh 1.30, thresh ineqal: gt, the weighted error is 0.714\n",
      "split: dim 0, thresh 1.40, thresh ineqal: lt, the weighted error is 0.286\n",
      "split: dim 0, thresh 1.40, thresh ineqal: gt, the weighted error is 0.714\n",
      "split: dim 0, thresh 1.50, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.50, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.60, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.60, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.70, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.70, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.80, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.80, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.90, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.90, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 2.00, thresh ineqal: lt, the weighted error is 0.857\n",
      "split: dim 0, thresh 2.00, thresh ineqal: gt, the weighted error is 0.143\n",
      "split: dim 1, thresh 0.89, thresh ineqal: lt, the weighted error is 0.143\n",
      "split: dim 1, thresh 0.89, thresh ineqal: gt, the weighted error is 0.857\n",
      "split: dim 1, thresh 1.00, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.00, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.11, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.11, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.22, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.22, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.33, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.33, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.44, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.44, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.55, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.55, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.66, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.66, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.77, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.77, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.88, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.88, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.99, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.99, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 2.10, thresh ineqal: lt, the weighted error is 0.857\n",
      "split: dim 1, thresh 2.10, thresh ineqal: gt, the weighted error is 0.143\n",
      "D: [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]\n",
      "classEst:  [[1. 1. 1. 1. 1.]]\n",
      "aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "total error:  0.0\n",
      "[{'dim': 0, 'thresh': 1.3, 'ineq': 'lt', 'alpha': 0.6931471805599453}, {'dim': 1, 'thresh': 1.0, 'ineq': 'lt', 'alpha': 0.9729550745276565}, {'dim': 0, 'thresh': 0.9, 'ineq': 'lt', 'alpha': 0.8958797346140273}]\n",
      "[[ 1.17568763]\n",
      " [ 2.56198199]\n",
      " [-0.77022252]\n",
      " [-0.77022252]\n",
      " [ 0.61607184]]\n"
     ]
    }
   ],
   "source": [
    "def adaBoostTrainDS(dataArr, classLabels, numIt = 40):\n",
    "    weakClassArr = []\n",
    "    m = np.shape(dataArr)[0]\n",
    "    D = np.mat(np.ones((m, 1)) / m)                                            #初始化权重\n",
    "    aggClassEst = np.mat(np.zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)     #构建单层决策树\n",
    "        print(\"D:\",D.T)\n",
    "        alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))         #计算弱学习算法权重alpha,使error不等于0,因为分母不能为0\n",
    "        bestStump['alpha'] = alpha                                          #存储弱学习算法权重\n",
    "        weakClassArr.append(bestStump)                                      #存储单层决策树\n",
    "        print(\"classEst: \", classEst.T)\n",
    "        expon = np.multiply(-1 * alpha * np.mat(classLabels).T, classEst)     #计算e的指数项\n",
    "        D = np.multiply(D, np.exp(expon))                                      \n",
    "        D = D / D.sum()                                                        #根据样本权重公式，更新样本权重\n",
    "        #计算AdaBoost误差，当误差为0的时候，退出循环\n",
    "        aggClassEst += alpha * classEst                                 \n",
    "        print(\"aggClassEst: \", aggClassEst.T)\n",
    "        aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classLabels).T, np.ones((m,1)))     #计算误差\n",
    "        errorRate = aggErrors.sum() / m\n",
    "        print(\"total error: \", errorRate)\n",
    "        if errorRate == 0.0: break                                             #误差为0，退出循环\n",
    "    return weakClassArr, aggClassEst\n",
    "\n",
    "dataArr,classLabels = loadSimpData()\n",
    "weakClassArr, aggClassEst = adaBoostTrainDS(dataArr, classLabels)\n",
    "print(weakClassArr)\n",
    "print(aggClassEst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454617a2",
   "metadata": {},
   "source": [
    "## 7.5 非均衡问题\n",
    "\n",
    "在大多数情况下不同类别的分类代价并不相等。比如有的时候我们会更在意是否“错杀”，或者有“遗漏”。有在本节中，我们将会考察一种新的分类器性能度量方法。\n",
    "\n",
    "### 7.5.1 正确率、召回率\n",
    "\n",
    "到现在为止，本书都是基于错误率：所有测试样例中错分的样例比例来衡量分类器任务的成功程度的。实际上，这样的度量错误掩盖了样例如何被分错的事实。\n",
    "\n",
    "在机器学习中，有一个普遍适用的称为混淆矩阵（confusion matrix）的工具，它可以帮助人们更好地了解分类中的错误。\n",
    "\n",
    "下图是一个二分类问题的混淆矩阵：\n",
    "\n",
    "\n",
    "\n",
    "|          |      | 预测结果     | 预测结果     |\n",
    "| -------- | ---- | ------------ | ------------ |\n",
    "|          |      | +1           | -1           |\n",
    "| 真实结果 | +1   | 真正例（TP） | 伪反例（FN） |\n",
    "| 真实结果 | -1   | 伪正例（FP） | 真反例（TN） |\n",
    "\n",
    "\n",
    "在分类中，当某个类别的重要性高于其他类别时，我们就可以利用上述定义来定义出多个比错误率更好的新指标。第一个指标是正确率,给出的是预测为正例的样本中的真正正例的比例.\n",
    "$$\n",
    "正确率Precision = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "第二个指标是召回率，给出的是预测为正例的真实正例占所有真实正例的比例。在召回率很大的分类器中，真正判错的正例的数目并不多。 \n",
    "$$\n",
    "召回率Recall = \\frac{TP}{TP+FN}\n",
    "$$\n",
    "\n",
    "我们可以很容易构造一个高正确率或高召回率的分类器，但是很难同时保证两者成立。如果将任何样本都判为正例，那么召回率达到百分之百而此时正确率很低。构建一个同时使正确率和召回率最大的分类器是具有挑战性的。\n",
    "\n",
    "### 7.5.2 ROC曲线\n",
    "\n",
    "横轴是伪正例的比例（假阳率=FP/(FP+TN)），而纵轴是真正例的比例（真阳率=TP/(TP+FN)）。ROC曲线给出的是当阈值\n",
    "变化时假阳率和真阳率的变化情况。左下角的点所对应的是将所有样例判为反例的情况，而右上角的点对应的则是将所有样例判为正例的情况。$y=x$直线，给出的是随机猜测的结果。\n",
    "\n",
    "ROC曲线不但可以用于比较分类器，还可以基于成本效益（cost-versus-benefit）分析来做出决策。由于在不同的阈值下，不同的分类器的表现情况可能各不相同，因此以某种方式将它们组合起来或许会更有意义。如果只是简单地观察分类器的错误率，那么我们就难以得到这种更深入\n",
    "的洞察效果了。 \n",
    "\n",
    "在理想的情况下，最佳的分类器应该尽可能地处于左上角，这就意味着分类器在假阳率很低的同时获得了很高的真阳率。\n",
    "\n",
    "对不同的ROC曲线进行比较的一个指标是曲线下的面积（Area Unser the Curve，AUC）。AUC给出的是分类器的平均性能值，当然它并不能完全代替对整条曲线的观察。一个完美分类器的AUC为1.0，而随机猜测的AUC则为0.5。\n",
    "\n",
    "### 7.5.3 损失函数调整\n",
    "\n",
    "除了调节分类器的阈值之外，我们还有一些其他可以用于处理非均衡分类代价的方法，其中的一种称为代价敏感的学习（cost-sensitive learning）。\n",
    "\n",
    "在分类算法中，我们有很多方法可以用来引入代价信息。在AdaBoost中，可以基于代价函数来调整错误权重向量D。在朴素贝叶斯中，可以选择具有最小期望代价而不是最大概率的类别作为最后的结果。在SVM中，可以在代价函数中对于不同的类别选择不同的参数C。上述做法就会给较小类更多的权重，即在训练时，小类当中只允许更少的错误。\n",
    "\n",
    "## 7.6 本章小结\n",
    "\n",
    "本章我们使用非常简单的弱分类器，根据错误率给当前分类器设定权重；根据样本预测的正确与否，对样本进行权重区分，然后在新权重样本下进行新一轮的预测，重复直到错误率=0或达到指定的条件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a338b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
