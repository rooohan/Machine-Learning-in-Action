{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e044d9",
   "metadata": {},
   "source": [
    "# 第4章 朴素贝叶斯\n",
    "\n",
    "前两章，我们要求分类器做出艰难决策，但是分类器又实惠产生错误结果。这时我们可以要求分类器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。\n",
    "\n",
    "在本章从最简单的**概率分类器**开始，然后给出一些假设来学习**朴素贝叶斯分类器**。这里的朴素是指：整个形式化过程只做最原始、最简单的假设。\n",
    "\n",
    "## 4.1 基于贝叶斯决策的分类方法\n",
    "\n",
    "> 朴素贝叶斯\n",
    ">\n",
    "> - 优点：在数据较少的情况下仍然有效，可以处理多类别问题。\n",
    "> - 缺点：对于输入数据的准备方式较为敏感。\n",
    "> - 适用类型：标称型数据(一般在有限的数据中取，而且只存在'是'和'否'两种不同的结果)。\n",
    "\n",
    "朴素贝叶斯，是贝叶斯决策理论的一部分，那么什么是贝叶斯决策理论呢？\n",
    "\n",
    "对于新数据，代入根据已知数据计算出来的概率函数，取各概率函数结果最大的为新数据的分类。\n",
    "\n",
    "## 4.2 条件概率\n",
    "\n",
    "对于一个事件$A$，它在统计学上有以下与概率相关的事件：\n",
    "\n",
    "- 随机事件$A$发生的**先验概率**$P(A)$：\n",
    "\n",
    "  $P(A)=\\frac{\\#(A)}{\\#}$\n",
    "\n",
    "- 随机事件$A$和随机事件$B$同时发生的**联合概率**：\n",
    "\n",
    "  $P(A,B)=P(A|B)·P(B)=P(B|A)·P(A)$\n",
    "\n",
    "  当$A B$相互独立，即$P(A|B)=P(A)$时，$P(A,B)=P(A)·P(B)$\n",
    "\n",
    "- 给定事件$B$已发生的条件下，$A$发生的**条件概率**：\n",
    "\n",
    "  $P(A|B)=\\frac{P(A,B)}{P(B)}$\n",
    "\n",
    "> 以3B1B视频中的提到的为例：\n",
    ">\n",
    "> 已知有农民200人，图书馆管理员10人。其中图书馆管理员中文质彬彬的人群有4人，农民中有20人。那么请问已知证据（Evidence）：一个人是文质彬彬的性格，那么假设（Hypothesis）“这个人是图书馆管理员的概率”$P(h|E)$是多少？\n",
    ">\n",
    ">  我们可以直接得出的结论有：\n",
    ">\n",
    "> 一个人是图书管理员的先验概率(prior)：$P(H)=\\frac{10}{10+200}=\\frac{1}{21}$\n",
    ">\n",
    "> 一个人文质彬彬的先验概率：$P(E)=\\frac{4+20}{10+200}$\n",
    ">\n",
    "> 文质彬彬时，是管理员的条件概率：$P(H|E)=\\frac{4}{4+20}$\n",
    ">\n",
    "> 文质彬彬时，是农民的条件概率：$P(\\neg H|E)=\\frac{20}{4+20}$\n",
    ">\n",
    "> ---\n",
    ">\n",
    "> 基于上面的信息，继续做的推论有：\n",
    ">\n",
    "> 既是管理员，又文质彬彬的联合概率：$P(H,E)=\\frac{4}{210}$\n",
    ">\n",
    "> 既文质彬彬又是管理员的联合概率：$P(E,H)=$\n",
    "\n",
    "[扩展阅读](https://0x100.club/math/bayes.html)\n",
    "\n",
    "总结：\n",
    "\n",
    "联合概率，在样本空间不变的情况下，使发生的条件更加苛刻了（如果联合的事件完全独立，那么就变成了不可能事件；）缩小了分子的取值，所以概率会变低；\n",
    "\n",
    "条件概率，限制了事件发生的样本空间，所以概率既可能变大也可能变小。\n",
    "\n",
    " 由联合概率公式，我们可以推出贝叶斯公式：\n",
    "$$\n",
    "P(A|B)=\\frac{P(B|A)·P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "## 4.3 使用条件概率分类\n",
    "\n",
    "根据贝叶斯公式，我们对于输入的信息$X$，判断其对应的分类，可得：\n",
    "$$\n",
    "P(Y|X)=\\frac{P(X|Y)·P(Y)}{P(X)}=\\frac{P(X|Y)}{P(X)}·P(Y)\n",
    "$$\n",
    "\n",
    "## 4.4 朴素贝叶斯进行分类\n",
    "\n",
    "朴素贝叶斯中的朴素（native）一词，是指：样本中的特征都是相互独立的，不考虑特征之间的相互影响。且每个特征同等重要。\n",
    "\n",
    "## 4.5 算法实现\n",
    "### 4.5.1 OneHot编码处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57c1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "# 返回数据集\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    #1 代表脏话, 0 not\n",
    "    classVec = [0,1,0,1,0,1]    \n",
    "    return postingList,classVec\n",
    "        \n",
    "# 对数据集去重，并返回新的数据集\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "# 接收词汇表 和要编码的数据，返回编码后的结果；是词袋模型，统计了每个词出现的次数\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "        else: \n",
    "            print(f\"the {word}: %s is not in my Vocabulary!\")\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62a9742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['take', 'dalmation', 'food', 'him', 'how', 'problems', 'dog', 'flea', 'to', 'park', 'cute', 'worthless', 'buying', 'has', 'stupid', 'my', 'stop', 'ate', 'maybe', 'is', 'mr', 'please', 'steak', 'help', 'not', 'posting', 'licks', 'I', 'quit', 'garbage', 'love', 'so']\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集和标签\n",
    "list_posts,list_classes = loadDataSet()\n",
    "# 返回不重复的词列表\n",
    "list_vocab = createVocabList(list_posts)\n",
    "print(list_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fb254c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此时的数据集格式已经变成:\n",
      "[[0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "list_mat = []\n",
    "for line in list_posts:\n",
    "    # 将数据集的每一行按照list_vocab，转化为向量\n",
    "    list_mat.append(setOfWords2Vec(list_vocab,line))\n",
    "np_mat = array(list_mat)\n",
    "print(f\"\"\"此时的数据集格式已经变成:\n",
    "{np_mat}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baddb1ec",
   "metadata": {},
   "source": [
    "至此，我们的准备工作已经完成。根据贝叶斯公式，我们需要计算几个概率，代码如下:\n",
    "\n",
    "todo：待补充要计算的概率是什么，以及推导过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7660c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e0b69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    \"\"\"\n",
    "    输入文档矩阵trainMatrix 和对应的标签向量trainCategory；\n",
    "    返回onehot编码后的各个位置上的，给定是1或者0的各个单词的条件概率的对数\n",
    "    \"\"\"\n",
    "    # 样本数量\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 每个样本的长度\n",
    "    numWords = len(trainMatrix[0])\n",
    "    \n",
    "    # 求出了负面评价的先验概率\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    \n",
    "    # 声明与样本长度一致的全1 array，用来累加某个词出现的频率\n",
    "    # 全1是避免：某个词出现次数为0，求条件概率时为0，导致联合概率也为0\n",
    "    # 这也称为拉普拉斯平滑\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)\n",
    "    # 同样也是为了避免概率为0\n",
    "    p0Denom = 2.0; p1Denom = 2.0 \n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        # 条件概率，限制分母的\n",
    "        if trainCategory[i] == 1:\n",
    "            # 会在全为1的OneHot向量上，做向量加法，即各自激活的位置上+1\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 累加当前OneHot上词的数量，即样本空间的总数。\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 将条件概率乘法，通过对数函数转换为先ln再加法，避免因为数据小，精度丢失。\n",
    "    # 是向量类型，贝叶斯公式右边的P(B|A)\n",
    "    p1Vect = log(p1Num/p1Denom)          \n",
    "    p0Vect = log(p0Num/p0Denom)          \n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ba952e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.25809654 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936\n",
      " -3.25809654 -2.56494936 -3.25809654 -3.25809654 -2.56494936 -1.87180218\n",
      " -2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.15948425 -3.25809654 -2.56494936\n",
      " -3.25809654 -2.56494936]\n",
      "[-1.65822808 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -2.35137526 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
      " -2.35137526 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -2.35137526 -3.04452244 -2.35137526 -2.35137526 -1.94591015\n",
      " -1.94591015 -3.04452244]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# 返回OneHot向量上，给定条件是积极的或者消极的，下不同词出现的频率\n",
    "vec_p0,vec_p1,pro_abusive = trainNB0(list_mat, list_classes)\n",
    "print(vec_p0)\n",
    "print(vec_p1)\n",
    "print(pro_abusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fabf7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    # vec2Classify是OneHot后的待预测数据，做乘法，可以理解为将OneHot上为1的位置，激活对应的条件概率\n",
    "    # 做加法是因为 条件概率 是取Ln的，所以其实就是假定各自独立的联合概率分布\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfcb08e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    \n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    \n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "\n",
    "    testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7971a",
   "metadata": {},
   "source": [
    "## 4.6 本章小结\n",
    "\n",
    "本章学习了朴素贝叶斯的分类器，首先是要假定样本的特征都是相互独立的，这也是“朴素”一词的意思。\n",
    "\n",
    "以垃圾邮件分类为例：\n",
    "1. 根据已有样本，首先对文本One-Hot编码\n",
    "2. 然后可以计算已知是垃圾邮件或者非垃圾邮件的条件下，每个单词出现的频率向量$v$（为了避免出现0，要假定各个单词最少出现一次，避免精度丢失，所以取log）\n",
    "3. 将未知数据One-Hot编码后，乘以$v$，对结果求和，并加上$log(P(A))$（因为取了对数，这两个加法其实都是乘法）\n",
    "4. 在代码中，其实还少了要除以$P(B)$的步骤，因为不论是求哪种情况，都需要除以$P(B)$，所以代码中省略了，对结果没有影响。即我不需要计算未知样本的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad877c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
